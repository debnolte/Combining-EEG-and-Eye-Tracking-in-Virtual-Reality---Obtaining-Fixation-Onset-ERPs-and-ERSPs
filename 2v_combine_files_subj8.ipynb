{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d514b315-f933-4f8a-9384-2ca04ad5cece",
   "metadata": {},
   "source": [
    "# Create one DataFrame out of the split participant 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb984b7-061f-4323-85e2-aeff733d5050",
   "metadata": {},
   "source": [
    "__Data Saved: (same as in 1v but only for participant 8)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84b102-7ad0-4e2f-afaf-9be940026524",
   "metadata": {},
   "source": [
    "uid == \"19a33ac1-149b-407c-a506-a2c7f4f3fea1\"\n",
    "- __Timestamps_try_uid.json__ --> json with substracted start from most behavioral timestamps\n",
    "- __Timestamps_new_uid.json__ --> json with sorted timestamps (to match ETW)\n",
    "- __Timestamps_misses_uid.json__ --> all the timestamps not matching ETW (mostly to check for errors)\n",
    "- __Timestamps_overall_uid.json__ --> one timestream all other streams share\n",
    "- __Behavior_new_uid.csv__ --> csv with behavioral columns, all with the same timestamps \n",
    "- __HitInfo_new_uid_raw.csv__ --> save all HitInfo with the same timestamp (for each timepoint save 30 rows, most are nulls)\n",
    "- __HitInfo_new_uid.csv__ --> save HitInfo same as before but only rows that are not null\n",
    "- __HitDistance_new_uid.csv__ --> save for each entry the smallest hit distance \n",
    "- __HitsSorted_new_uid.csv__ --> save the closest distance more HitInfo (fo sorted HitInfo_new df based on distance)\n",
    "- __Timestamps_overall_uid.json__ --> one timestream all other streams share\n",
    "- __recordings_village.csv__ --> we save the ids + length of all recordings in one csv (with corrected participant 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68808a-5582-45f2-baa2-255ad6287baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy  # copy big/deep objects by value\n",
    "import datetime  # datetime operations\n",
    "import itertools  # operate with iterators\n",
    "import json  # read/write from/into json format\n",
    "import os  # OS operations (read/write files/folders)\n",
    "import uuid\n",
    "import warnings  # hide warnings\n",
    "# process parallelization\n",
    "from multiprocessing import Manager, Pool, RawArray, cpu_count\n",
    "\n",
    "import matplotlib.pyplot as plt  # mother of plots focr Python\n",
    "import numpy as np  # array/matrix operations (e.g. linear algebra)\n",
    "import pandas as pd  # operate with dataframes\n",
    "import pyxdf  # read XDF files (LSL streams recordings)\n",
    "import seaborn as sns  # matplotlib plotting nice with shortcuts\n",
    "from IPython.display import Markdown, display  # print nicely\n",
    "from tqdm.notebook import tqdm  # mother of progressbars\n",
    "\n",
    "# from matplotlib.ticker import FormatStrFormatter  # tick formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001703c-6bab-48af-b576-8ff0e901d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# raw and processed data paths\n",
    "PATH_RAW = \"./data/raw\"\n",
    "PATH_PROC = \"./data/processed\"\n",
    "\n",
    "# dtypes specification to avoid dtype guessing warning\n",
    "CUSTOM_DTYPES = {\n",
    "    \"valid\": \"boolean\",\n",
    "    \"leftBlink\": \"boolean\",\n",
    "    \"rightBlink\": \"boolean\",\n",
    "}\n",
    "\n",
    "def create_concat_df(df1, df2):\n",
    "    \"\"\"\n",
    "    Given two dataframes and will return a combined one with the time axis corrected\n",
    "    df1 will be first, followed by df2\n",
    "    \"\"\"\n",
    "    times = df2.index.tolist()\n",
    "    to_add = df1.index.tolist()\n",
    "    to_add = to_add[-1] + 0.011\n",
    "    new_times = [round(t + to_add, 3) for t in times]\n",
    "\n",
    "    df3 = pd.concat([df1,df2], ignore_index=True)\n",
    "\n",
    "    df3[\"new_time\"] = df1.index.tolist() + new_times\n",
    "    df3 = df3.set_index('new_time', drop=True)\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d71b3e-5942-4a7f-acc8-dd59cb0c5963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_uid = str(uuid.uuid4()) # only run this once!!\n",
    "new_uid = \"19a33ac1-149b-407c-a506-a2c7f4f3fea1\"\n",
    "new_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cdc64d-c33f-4d62-bca0-b9481de60db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two files to combine are: \"08_v_100621.xdf\" followed by \"08_v2_100621.xdf\"\n",
    "idd = [\n",
    "    \"6aebc6c2-6a6a-4038-b729-fddfdd0418c0\",\n",
    "    \"ee9dac3c-a7e5-48b7-8187-6d9038651352\",\n",
    "]\n",
    "uid1 = \"6aebc6c2-6a6a-4038-b729-fddfdd0418c0\"\n",
    "uid2 = \"ee9dac3c-a7e5-48b7-8187-6d9038651352\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50c186-1e69-4247-a264-7334152451f3",
   "metadata": {},
   "source": [
    "__Create Combined DataFrames__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a938b78-764c-459d-89e2-38239e10894a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of all dataframes to change:\n",
    "# beh_data.to_csv(f\"{PATH_PROC}/Behavior_new_{uid}.csv\", index=True)\n",
    "# hit_data.to_csv(f\"{PATH_PROC}/HitInfo_new_{uid}_raw.csv\", index=True)\n",
    "# h_res.to_csv(f\"{PATH_PROC}/HitInfo_new_{uid}.csv\", index=True)\n",
    "# hits.to_csv(f\"{PATH_PROC}/HitDistance_new_{uid}.csv\", index=False)\n",
    "# hits_sorted.to_csv(f\"{PATH_PROC}/HitsSorted_new_{uid}.csv\", index=True)\n",
    "\n",
    "# Behavior_new:\n",
    "beh1 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/Behavior_new_{uid1}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "beh2 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/Behavior_new_{uid2}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "beh_new = create_concat_df(beh1, beh2)\n",
    "\n",
    "# HitInfo_new..._raw:\n",
    "hit_raw1 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/HitInfo_new_{uid1}_raw.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "hit_raw2 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/HitInfo_new_{uid2}_raw.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "hit_raw_new = create_concat_df(hit_raw1, hit_raw2)\n",
    "\n",
    "# HitInfo_new:\n",
    "hit1 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/HitInfo_new_{uid1}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "hit2 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/HitInfo_new_{uid2}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "hit_new = create_concat_df(hit1, hit2)\n",
    "\n",
    "# HitDistance_new:\n",
    "hit_dist1 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/HitDistance_new_{uid1}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "hit_dist2 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/HitDistance_new_{uid2}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "hit_dist_new = create_concat_df(hit_dist1, hit_dist2)\n",
    "\n",
    "# HitsSorted_new:\n",
    "hit_sort1 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/HitsSorted_new_{uid1}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "hit_sort2 = pd.read_csv(\n",
    "    f\"{PATH_PROC}/HitsSorted_new_{uid2}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "hit_sort_new = create_concat_df(hit_sort1, hit_sort2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745b7e8-bf8e-49b5-b159-ac6cf51046d8",
   "metadata": {},
   "source": [
    "__Check that created beh4 is correct__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0295122-cc2f-4007-9c1a-1d37223c968e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = hit_raw_new.index.tolist()\n",
    "t1 = hit_raw1.index.tolist()\n",
    "t2 = hit_raw2.index.tolist()\n",
    "len1 = seconds_to_minutes_seconds(t2[-1] - t2[0])\n",
    "print(len1)\n",
    "len2 = seconds_to_minutes_seconds(t1[-1] - t1[0])\n",
    "print(len2)\n",
    "length = seconds_to_minutes_seconds(ts[-1] - ts[0])\n",
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f67a5-7107-489b-aa3f-55738b253805",
   "metadata": {},
   "source": [
    "__Save the new DataFrames__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f2bbf-91ea-4eb6-89e9-7be6ada00875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beh_new.to_csv(f\"{PATH_PROC}/Behavior_new_{new_uid}.csv\", index=True)\n",
    "hit_raw_new.to_csv(f\"{PATH_PROC}/HitInfo_new_{new_uid}_raw.csv\", index=True)\n",
    "hit_new.to_csv(f\"{PATH_PROC}/HitInfo_new_{new_uid}.csv\", index=True)\n",
    "hit_dist_new.to_csv(f\"{PATH_PROC}/HitDistance_new_{new_uid}.csv\", index=True)\n",
    "hit_sort_new.to_csv(f\"{PATH_PROC}/HitsSorted_new_{new_uid}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d687e50-af89-4ac0-a80a-901762f587ba",
   "metadata": {},
   "source": [
    "__Create Timestamp_overall__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861e4f4-82bf-47fa-aaf5-d9c2fd674c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beh_new = pd.read_csv(\n",
    "    f\"{PATH_PROC}/Behavior_new_{new_uid}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    ")\n",
    "\n",
    "times_overall = beh_new.index.tolist()\n",
    "with open(f\"{PATH_PROC}/Timestamps_overall_{new_uid}.json\", \"w\") as f:\n",
    "    json.dump(times_overall, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd7a1c-522d-448c-8586-67424d3997e4",
   "metadata": {},
   "source": [
    "__Adjust the recording infromation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7d74f-d83d-4939-922a-c30e044b3b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings = pd.read_csv(\"./recordings_village_old.csv\", index_col=\"new_id\")\n",
    "ids = recordings.index.tolist()\n",
    "recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d82c9-8198-4c95-9996-2151b2407784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = beh_new.index.tolist()\n",
    "length = seconds_to_minutes_seconds(ts[-1] - ts[0])\n",
    "\n",
    "rec = {}\n",
    "rec[new_uid] = {}\n",
    "rec[new_uid][\"file\"] = \"08_v_comb.xdf\"\n",
    "rec[new_uid][\"created\"] = \"2021-06-10 14:12:00\"\n",
    "rec[new_uid][\"length\"] = length\n",
    "rec[new_uid][\"start\"] = 1.106542e06\n",
    "rec[new_uid] = pd.Series(rec[new_uid])\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9459b24-4846-43bf-a1a8-cdabdf966bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings_new = []\n",
    "idds = []\n",
    "for i in ids:\n",
    "    if i not in idd[:]:\n",
    "        idds.append(i)\n",
    "        recordings_new.append(recordings.loc[i])\n",
    "    elif uid2 in i:\n",
    "        recordings_new.append(rec[new_uid])\n",
    "        idds.append(new_uid)\n",
    "\n",
    "\n",
    "recordings_new = pd.DataFrame(recordings_new, index=idds)\n",
    "\n",
    "# store recordings info as CSV\n",
    "recordings_new.to_csv(\"./recordings_village.csv\", index=True)\n",
    "recordings_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wd_village)",
   "language": "python",
   "name": "wd_village"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
