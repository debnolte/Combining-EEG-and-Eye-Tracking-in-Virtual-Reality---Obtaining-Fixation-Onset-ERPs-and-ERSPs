{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code created together with Marc Vidal De Palol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing: Unify the timestream and create all necessary dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to free up memory\n",
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Saved:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: participant 8 is saved in two files, they are combined as one in 1.5v_.\n",
    "-  __Timestamps_try_uid.json__ --> json with substracted start from most behavioral timestamps\n",
    "-  __Timestamps_new_uid.json__ --> json with sorted timestamps (to match ETW)\n",
    "- __Timestamps_misses_uid.json__ --> all the timestamps not matching ETW (mostly to check for errors)\n",
    "- __Timestamps_overall_uid.json__ --> one timestream all other streams share\n",
    "- __Behavior_new_uid.csv__ --> csv with behavioral columns, all with the same timestamps \n",
    "- __HitInfo_new_uid_raw.csv__ --> save all HitInfo with the same timestamp (for each timepoint save 30 rows, most are nulls)\n",
    "- __HitInfo_new_uid.csv__ --> save HitInfo same as before but only rows that are not null\n",
    "- __HitDistance_new_uid.csv__ --> save for each entry the smallest hit distance \n",
    "- __HitsSorted_new_uid.csv__ --> save the closest distance more HitInfo (fo sorted HitInfo_new df based on distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dependencies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy  # copy big/deep objects by value\n",
    "import datetime  # datetime operations\n",
    "import itertools  # operate with iterators\n",
    "import json  # read/write from/into json format\n",
    "import os  # OS operations (read/write files/folders)\n",
    "import warnings  # hide warnings\n",
    "\n",
    "# process parallelization\n",
    "from multiprocessing import Manager, Pool, RawArray, cpu_count\n",
    "\n",
    "import matplotlib.pyplot as plt  # mother of plots for Python\n",
    "import numpy as np  # array/matrix operations (e.g. linear algebra)\n",
    "import pandas as pd  # operate with dataframes\n",
    "import pyxdf  # read XDF files (LSL streams recordings)\n",
    "import seaborn as sns  # matplotlib plotting nice with shortcuts\n",
    "from IPython.display import Markdown, display  # print nicely\n",
    "from tqdm.notebook import tqdm  # mother of progressbars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Options__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# raw and processed data paths\n",
    "PATH_RAW = \"./data/raw\"\n",
    "PATH_PROC = \"./data/processed\"\n",
    "\n",
    "# specify decimals format on pandas tables\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "\n",
    "STYLE = \"darkgrid\"\n",
    "sns.set_style(STYLE)  # set seaborn plotting style\n",
    "\n",
    "# progress bar customized format\n",
    "B_FORMAT = \"\"\"üìÑ {n_fmt} of {total_fmt} {desc} processed: {bar} \n",
    "            {percentage:3.0f}% ‚è±Ô∏è{elapsed} ‚è≥{remaining} ‚öôÔ∏è{rate_fmt}{postfix}\"\"\"\n",
    "\n",
    "\n",
    "# streams to remove\n",
    "REMOVE = [\n",
    "    \"openvibeSignal\",\n",
    "    \"openvibeMarkers\",\n",
    "    \"AgentRotation\",\n",
    "    \"StaticAgentPosition\",\n",
    "    \"StaticAgentRotation\",\n",
    "    \"AgentPosition\",\n",
    "    \"ValidationError\",\n",
    "    \"ButtonPresses\",\n",
    "    \"PlayerPosition\",\n",
    "]\n",
    "\n",
    "# custom order of streams (to process recordings on the same order)\n",
    "CUSTOM_ORDER = [\n",
    "    \"EyeTrackingWorld\",\n",
    "    \"HitObjectNames\",\n",
    "    \"HitObjectPositions\",\n",
    "    \"HitPositionOnObjects\",\n",
    "    \"EyeTrackingLocal\",\n",
    "    \"HeadTracking\",\n",
    "    \"HitObjectNamesHead\",\n",
    "    \"HitObjectPositionsHead\",\n",
    "    \"HitPositionOnObjectsHead\",\n",
    "]\n",
    "\n",
    "# custom order of Behavior dataframe columns\n",
    "BEH_COLS = [\n",
    "    \"valid\",\n",
    "    \"leftBlink\",\n",
    "    \"rightBlink\",\n",
    "    \"ETWTime\",\n",
    "    \"ETWoriginX\",\n",
    "    \"ETWoriginY\",\n",
    "    \"ETWoriginZ\",\n",
    "    \"ETWdirectionX\",\n",
    "    \"ETWdirectionY\",\n",
    "    \"ETWdirectionZ\",\n",
    "    \"HON\",\n",
    "    \"ETLoriginX\",\n",
    "    \"ETLoriginY\",\n",
    "    \"ETLoriginZ\",\n",
    "    \"ETLdirectionX\",\n",
    "    \"ETLdirectionY\",\n",
    "    \"ETLdirectionZ\",\n",
    "    \"HToriginX\",\n",
    "    \"HToriginY\",\n",
    "    \"HToriginZ\",\n",
    "    \"HTdirectionX\",\n",
    "    \"HTdirectionY\",\n",
    "    \"HTdirectionZ\",\n",
    "]\n",
    "\n",
    "# custom order of hit_cols df\n",
    "HIT_COLS = [\n",
    "    \"valid\",\n",
    "    \"leftBlink\",\n",
    "    \"rightBlink\",\n",
    "    \"HON\",\n",
    "    \"HOPX\",\n",
    "    \"HOPY\",\n",
    "    \"HOPZ\",\n",
    "    \"HPOOX\",\n",
    "    \"HPOOY\",\n",
    "    \"HPOOZ\",\n",
    "]\n",
    "# custom order of hit_sort df\n",
    "HIT_SORT = [\n",
    "    \"valid\",\n",
    "    \"leftBlink\",\n",
    "    \"rightBlink\",\n",
    "    \"HON\",\n",
    "    \"distance\",\n",
    "    \"HOPX\",\n",
    "    \"HOPY\",\n",
    "    \"HOPZ\",\n",
    "    \"HPOOX\",\n",
    "    \"HPOOY\",\n",
    "    \"HPOOZ\",\n",
    "]\n",
    "\n",
    "# dtypes specification to avoid dtype guessing warning\n",
    "CUSTOM_DTYPES = {\n",
    "    \"valid\": \"boolean\",\n",
    "    \"leftBlink\": \"boolean\",\n",
    "    \"rightBlink\": \"boolean\",\n",
    "}\n",
    "\n",
    "CORES = cpu_count()  # number of cpu threads for multiprocessing\n",
    "print(f\"Total CPU threads: {CORES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    \"\"\"Print string nicely using Markdown syntax.\"\"\"\n",
    "    display(Markdown(string))\n",
    "\n",
    "\n",
    "def pbar_fork_hack():\n",
    "    \"\"\"\n",
    "    Hack to enforce progress bars to be displayed by fork processes on\n",
    "    IPython Apps like Jupyter Notebooks.\n",
    "\n",
    "    Avoids [IPKernelApp] WARNING | WARNING: attempted to send message from fork\n",
    "\n",
    "    Important: pass this function as argument for the initializer parameter\n",
    "    while initializing a multiprocessing pool to make it work. E.g.:\n",
    "\n",
    "    pool = Pool(processes=N_CORES, initializer=pbar_fork_hack)\n",
    "\n",
    "    Source:\n",
    "     - https://github.com/ipython/ipython/issues/11049#issue-306086846\n",
    "     - https://github.com/tqdm/tqdm/issues/485#issuecomment-473338308\n",
    "    \"\"\"\n",
    "    print(\" \", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recordings info__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the df created in script 0v\n",
    "recordings = pd.read_csv(\"./recordings_village_old.csv\", index_col=0)\n",
    "recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Participant ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# participants ids\n",
    "ids = recordings.index.tolist()\n",
    "old_id = recordings[\"old_id\"].tolist() # old_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recording overview:__ display the different streams recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_labels(uid):\n",
    "    \"\"\"\n",
    "    Display stream labels (if any) given a participant id.\n",
    "\n",
    "    Parameters:\n",
    "        uid (str): Participant identifier.\n",
    "    \"\"\"\n",
    "    # get the recording filename associated with the participant id\n",
    "    part = recordings.loc[uid].file\n",
    "    # to store filename and uid\n",
    "    res = f\"__Recording `{part} - {uid}`__<br>\"\n",
    "    # load XDF data for the specified recording\n",
    "    data, _ = pyxdf.load_xdf(f\"{PATH_RAW}/{part}\")\n",
    "    # iterate over each stream in the recording\n",
    "    for s in data:\n",
    "        # get the stream name\n",
    "        s_name = s[\"info\"][\"name\"][0]\n",
    "        # include stream name in the result string\n",
    "        res += f\"_{s_name}_: \"\n",
    "        # check that the stream is not 'openvibe' (EEG)\n",
    "        if \"openvibe\" not in s_name:\n",
    "            # extract stream labels (as specified in Unity) from the description \n",
    "            check = s[\"info\"][\"desc\"][0]\n",
    "            labels = None if not check else list(check.keys())\n",
    "        else:\n",
    "            # if stream is 'openvibe, set labels to \"None\"\n",
    "            labels = \"None\"\n",
    "        # include stream labels in the result string\n",
    "        res += f\"{labels}<br>\"\n",
    "    printmd(res)  # display them nicely\n",
    "\n",
    "\n",
    "# initialize pool of processes according to the available cpu core threads\n",
    "pool = Pool(processes=CORES, initializer=pbar_fork_hack)\n",
    "\n",
    "# get the list of participants from the reocrdings df\n",
    "parts = recordings.index.tolist()\n",
    "parts = parts[:]\n",
    "\n",
    "# recordings progress bar\n",
    "recs_pbar = tqdm(\n",
    "    iterable=pool.imap(func=read_labels, iterable=parts),\n",
    "    total=len(parts),\n",
    "    desc=\"üìº recordings\",\n",
    "    dynamic_ncols=True,\n",
    "    bar_format=B_FORMAT,\n",
    ")\n",
    "\n",
    "# loop necessary for displaying properly the progressbar with multiprocessing\n",
    "# source: https://stackoverflow.com/a/40133278\n",
    "for _ in recs_pbar:\n",
    "    pass\n",
    "\n",
    "# close pool instance, no more work to submit\n",
    "pool.close()\n",
    "# wait for the worker processes to terminate\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Subestract Start from all Streams__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get list of participants ids\n",
    "ids = recordings.index.tolist()\n",
    "idd = ids[:]\n",
    "old_id = recordings[\"old_id\"].tolist()\n",
    "\n",
    "# iterate over participants\n",
    "for u in range(len(idd)):\n",
    "    uid = idd[u] # get current id\n",
    "    oid = old_id[u] # old id\n",
    "    \n",
    "    # get the name of the file associated with the uid\n",
    "    fname = recordings.loc[uid].file\n",
    "    # get the start timestamp of the recording\n",
    "    start = round(recordings.loc[uid].start, 3)\n",
    "    \n",
    "    # load XDF data for the participant\n",
    "    data, _ = pyxdf.load_xdf(f\"{PATH_RAW}/{fname}\")\n",
    "\n",
    "    # remove streams specified in the REMOVE list (EEG + streams with irregular SR)\n",
    "    data = [d for d in data if d[\"info\"][\"name\"][0] not in REMOVE]\n",
    "\n",
    "    # streams progress bar\n",
    "    streams_pbar = tqdm(\n",
    "        iterable=data,\n",
    "        desc=f\"üßª streams from participant {uid}\",\n",
    "        dynamic_ncols=True,\n",
    "        bar_format=B_FORMAT,\n",
    "    )\n",
    "\n",
    "    # collect the beginning of each stream\n",
    "    starts = []\n",
    "    for st in streams_pbar:  # iterate over each stream\n",
    "        # to ensure the data is no in \"openvibeMarkers\"\n",
    "        if st[\"info\"][\"name\"][0] not in [\"openvibeMarkers\"]:\n",
    "            starts.append(st[\"time_stamps\"][0])\n",
    "    # get the minimum start of all streams\n",
    "    to_sub = min(starts)\n",
    "\n",
    "    # prepare a dictionary to store sorted timestamps for each stream\n",
    "    sorted_out = {}\n",
    "    # iterate over each stream\n",
    "    for s in streams_pbar:\n",
    "        if st[\"info\"][\"name\"][0] not in [\"openvibeMarkers\"]:\n",
    "            # get the current time stamps\n",
    "            times = s[\"time_stamps\"]\n",
    "            s_name = s[\"info\"][\"name\"][0]\n",
    "\n",
    "            # substract the minimum timestamp across all streams from all \n",
    "            # timepoints of the current stream\n",
    "            time = times - to_sub\n",
    "            # round them to three points after the comma (ms precision)\n",
    "            sorted_out[s_name] = [round(t, 3) for t in time]\n",
    "\n",
    "    # store the sorted timestamps to a JSON file for each participant\n",
    "    with open(f\"{PATH_PROC}/Timestamps_try_{uid}.json\", \"w\") as f:\n",
    "        json.dump(sorted_out, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sort Timestamps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sort Timestamps to match ETW, takes a while__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sort_timestamps(uid):\n",
    "    # get the data\n",
    "    fname = recordings.loc[uid].file\n",
    "    f = open(\n",
    "        f\"{PATH_PROC}/Timestamps_try_{uid}.json\", \"r\"\n",
    "    )  # this is the file where we are substracting start timestamps from all of them\n",
    "    times = json.load(f)  # load file content as JSON\n",
    "    f.close()\n",
    "    streams = list(times.keys())\n",
    "\n",
    "    # prefedined variables to save the data\n",
    "    new_times = {}  # save the newly ordered timestamps\n",
    "    check_misses = {} # save the timestamps where we are using the original and not the new ones\n",
    "\n",
    "    # get the first time stamp for each stream and the ETW stream\n",
    "    for s in streams:\n",
    "        if s == \"EyeTrackingWorld\":\n",
    "            common_time = times[s]\n",
    "            new_times[s] = times[s]\n",
    "            check_misses[s] = []\n",
    "\n",
    "    # streams progress bar\n",
    "    streams_pbar = tqdm(\n",
    "        iterable=streams,\n",
    "        desc=f\"üßª streams from participant {uid}\",\n",
    "        dynamic_ncols=True,\n",
    "        bar_format=B_FORMAT,\n",
    "    )\n",
    "\n",
    "    for s_name in streams_pbar:\n",
    "        if (s_name not in REMOVE) and (s_name not in Check):\n",
    "            # we want to match each stream ETW since this is the first stream in the eye-tracking file that should get saved\n",
    "            # to add the reordered timestamps\n",
    "            tim = []  # save the current timestamps in\n",
    "            chm = []  # save the timestamps in should they not come from ETW\n",
    "            comm = []\n",
    "            start_world = 0 # gets counted up if there is a world point\n",
    "            # go through all timestamps and check for matching ones\n",
    "            for t in times[s_name]:\n",
    "                # gets set to true if you found the corresponding stream\n",
    "                start_stream = False  # this has to be checked for each element in times[s_name] as we want all the streams to be included\n",
    "                # first check: see if there is an exact match\n",
    "                if t in common_time:\n",
    "                    # if there is an exact match, save that time stamp\n",
    "                    # if there is more than one element .index() returns the first one in the list\n",
    "                    tim.append(t)\n",
    "                    # set the start to the current match\n",
    "                    start_world = common_time.index(t)\n",
    "                    start_stream = True\n",
    "                # if it is still smaller than compare time  times[s_name][t]\n",
    "                else:\n",
    "                    for et in range(start_world, len(common_time)):\n",
    "                        # if the distance between both is small enough, change it to ETW timestamp --> here direction does not matter\n",
    "                        if (abs(common_time[et] - t) < 0.005) and (\n",
    "                            start_stream == False\n",
    "                        ):\n",
    "                            tim.append(common_time[et])\n",
    "                            start_stream = True\n",
    "                            start_world = et\n",
    "                            break\n",
    "                    # if there are no time stamps left of ETW just add the current ones\n",
    "                    if start_stream == False:\n",
    "                        tim.append(t)\n",
    "                        chm.append(t)\n",
    "                        start_stream = True\n",
    "                        comm.append(t)\n",
    "\n",
    "            common_time = common_time + comm\n",
    "            common_time.sort()\n",
    "\n",
    "            if s_name not in Check:\n",
    "                # after going through all of the timestamps of one stream, \n",
    "                # add the new timestamps to the variable that will be saved\n",
    "                new_times[s_name] = tim  # adding new re-ordered timestamps\n",
    "                check_misses[s_name] = chm  # adding the indices that were missed\n",
    "\n",
    "\n",
    "    # save both the new timestamps as well as the missed ones\n",
    "    with open(f\"{PATH_PROC}/Timestamps_new_{uid}.json\", \"w\") as f:\n",
    "        json.dump(new_times, f, indent=4)\n",
    "    with open(f\"{PATH_PROC}/Timestamps_misses_{uid}.json\", \"w\") as f:\n",
    "        json.dump(check_misses, f, indent=4)\n",
    "    with open(f\"{PATH_PROC}/Timestamps_overall_{uid}.json\", \"w\") as f:\n",
    "        json.dump(common_time, f, indent=4)\n",
    "\n",
    "\n",
    "# to be checked\n",
    "Check = [\n",
    "    \"EyeTrackingWorld\",\n",
    "]\n",
    "\n",
    "\n",
    "# initialize pool of processes according to the available cpu core threads\n",
    "pool = Pool(processes=CORES, initializer=pbar_fork_hack)\n",
    "\n",
    "# participants ids\n",
    "ids = recordings.index.tolist()\n",
    "idd = ids[:]\n",
    "\n",
    "# participants progress bar\n",
    "parts_pbar = tqdm(\n",
    "    iterable=pool.imap_unordered(func=sort_timestamps, iterable=idd),\n",
    "    total=len(idd),\n",
    "    desc=\"üìÇ participants\",\n",
    "    dynamic_ncols=True,\n",
    "    bar_format=B_FORMAT,\n",
    ")\n",
    "\n",
    "# loop necessary for displaying properly the progressbar with multiprocessing\n",
    "# source: https://stackoverflow.com/a/40133278\n",
    "for _ in parts_pbar:\n",
    "    pass\n",
    "\n",
    "# close pool instance, no more work to submit\n",
    "pool.close()\n",
    "# wait for the worker processes to terminate\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create Beahavioral df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Putting all behavioral data into one csv file (expect HON etc.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sort_data(uid):\n",
    "    # load data: raw data\n",
    "    part = recordings.loc[uid].file\n",
    "    data, _ = pyxdf.load_xdf(f\"{PATH_RAW}/{part}\")\n",
    "    # remove non relevant streams specified in REMOVE list\n",
    "    data = [d for d in data if d[\"info\"][\"name\"][0] not in REMOVE]\n",
    "\n",
    "    # load data: new timestream\n",
    "    fname = recordings.loc[uid].file\n",
    "    f = open(f\"{PATH_PROC}/Timestamps_new_{uid}.json\", \"r\")\n",
    "    times = json.load(f)  # load file content as JSON\n",
    "    f.close()\n",
    "\n",
    "    # load data: common timestream\n",
    "    f = open(f\"{PATH_PROC}/Timestamps_overall_{uid}.json\", \"r\")\n",
    "    times_overall = json.load(f)  # load file content as JSON\n",
    "    f.close()\n",
    "\n",
    "    # streams progress bar\n",
    "    streams_pbar = tqdm(\n",
    "        iterable=data,\n",
    "        desc=f\"üßª streams from participant {uid}\",\n",
    "        dynamic_ncols=True,\n",
    "        bar_format=B_FORMAT,\n",
    "    )\n",
    "\n",
    "    check_if = False  # used later for concatentings dataframes\n",
    "    \n",
    "    # go through all the streams and try to match them to the common_time, otherwise add nan\n",
    "    for data_s in streams_pbar:\n",
    "        s_name = data_s[\"info\"][\"name\"][\n",
    "            0\n",
    "        ]  # first, get the name of the current stream\n",
    "        # prepare  for the lists:\n",
    "        beh_adding = [] # list to store the adjusted behavioral data\n",
    "        original_time = [] # list to store the original timestamps\n",
    "        # only do stuff if the current stream is not in IGNORE:\n",
    "        if s_name not in IGNORE:\n",
    "            original_time = times[\n",
    "                s_name\n",
    "            ]  # get the time of the current stream that you want to adjust\n",
    "            count_t = 0 # counter to keep track of the index in original_time list\n",
    "            # go through every timestamp\n",
    "            for t_time in range(len(times_overall)):\n",
    "                t = times_overall[\n",
    "                    t_time\n",
    "                ]  # get the actual timestamp not just the index\n",
    "                found_match = False  # this is used to check if the current timestamp was already matched\n",
    "                \n",
    "                for i in range(len(original_time)):\n",
    "                    # so if the current timestamp is part of the original dataset, add the values\n",
    "                    if original_time[i] == t:\n",
    "                        # hitobjectnames: add hit object or empty\n",
    "                        if s_name in \"HitObjectNames\":\n",
    "                            if data_s[\"time_series\"][i][0] == \"Empty\":\n",
    "                                beh_adding.append(\"Empty\")\n",
    "                            else:\n",
    "                                beh_adding.append(\"ObjectNames\")\n",
    "                        # else: add the values\n",
    "                        else:\n",
    "                            beh_adding.append(data_s[\"time_series\"][i])\n",
    "                        count_t = i  # count this up\n",
    "                        found_match = True  # indicate that we found a match\n",
    "                        break\n",
    "                # if no match was found for the current timestamp, add NaN to the list\n",
    "                if found_match == False:\n",
    "                    if s_name in \"HitObjectNames\":\n",
    "                        beh_adding.append(np.nan)\n",
    "                    else:\n",
    "                        to_add = np.empty(len(data_s[\"time_series\"][count_t]))\n",
    "                        to_add[:] = np.nan\n",
    "                        beh_adding.append(list(to_add))\n",
    "\n",
    "            # get the name of the colums for the dataframe\n",
    "            check = data_s[\"info\"][\"desc\"][0]\n",
    "            labels = None if not check else list(check.keys())\n",
    "\n",
    "            # the first time you save data into dataframe (so for the first stream) you have to create it\n",
    "            if not check_if:\n",
    "                # the index are the column names (e.g. valid,...)\n",
    "                beh_data = pd.DataFrame(\n",
    "                    data=beh_adding, columns=labels, index=times_overall\n",
    "                )\n",
    "                # check_if is true so we don't overwrite excisting data\n",
    "                check_if = True\n",
    "            # once you already have the beh_data dataframe you have to concatenate the onld and new one\n",
    "            else:\n",
    "                c = pd.DataFrame(\n",
    "                    data=beh_adding, columns=labels, index=times_overall\n",
    "                )\n",
    "                beh_data = pd.concat([beh_data, c], axis=1, join=\"outer\")\n",
    "\n",
    "\n",
    "    # reorder the  dict:\n",
    "    beh_data = beh_data.reindex(columns=BEH_COLS)\n",
    "\n",
    "    # save the csv file\n",
    "    beh_data.to_csv(f\"{PATH_PROC}/Behavior_new_{uid}.csv\", index=True)\n",
    "\n",
    "# streams to be ingored \n",
    "IGNORE = [\n",
    "    \"HitObjectPositions\",\n",
    "    \"HitPositionOnObjects\",\n",
    "    \"HitObjectNamesHead\",\n",
    "    \"HitPositionOnObjectsHead\",\n",
    "    \"HitObjectPositionsHead\",\n",
    "]\n",
    "\n",
    "# initialize pool of processes according to the available cpu core threads\n",
    "pool = Pool(processes=CORES, initializer=pbar_fork_hack)\n",
    "\n",
    "# participants ids\n",
    "ids = recordings.index.tolist()\n",
    "idd = ids[18:]\n",
    "\n",
    "# participants progress bar\n",
    "parts_pbar = tqdm(\n",
    "    iterable=pool.imap_unordered(func=sort_data, iterable=idd),\n",
    "    total=len(idd),\n",
    "    desc=\"üìÇ participants\",\n",
    "    dynamic_ncols=True,\n",
    "    bar_format=B_FORMAT,\n",
    ")\n",
    "\n",
    "# loop necessary for displaying properly the progressbar with multiprocessing\n",
    "# source: https://stackoverflow.com/a/40133278\n",
    "for _ in parts_pbar:\n",
    "    pass\n",
    "\n",
    "# close pool instance, no more work to submit\n",
    "pool.close()\n",
    "# wait for the worker processes to terminate\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create Hit Info dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Hit-Info DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_hits(uid):\n",
    "    \"\"\"\n",
    "    Arrange, process and store hit information data given a recording id.\n",
    "\n",
    "    Parameters:\n",
    "        uid (str): Participant identifier.\n",
    "    \"\"\"\n",
    "    # load raw recording data\n",
    "    fname = recordings.loc[uid].file\n",
    "    data, _ = pyxdf.load_xdf(f\"{PATH_RAW}/{fname}\")\n",
    "\n",
    "    # select only streams to process\n",
    "    data = [d for d in data if d[\"info\"][\"name\"][0] in INCLUDE]\n",
    "\n",
    "    # reorder streams based on the order in INCLUDE list\n",
    "    data.sort(key=lambda d: INCLUDE.index(d[\"info\"][\"name\"][0]))\n",
    "\n",
    "    # read sorted timestamps of all streams (dict)\n",
    "    f = open(f\"{PATH_PROC}/Timestamps_new_{uid}.json\", \"r\")\n",
    "    times = json.load(f)  # load file content as JSON\n",
    "    f.close()\n",
    "\n",
    "    # open previously sorted behavioural data (also to include valid blinks etc.)\n",
    "    beh_data = pd.read_csv(\n",
    "        f\"{PATH_PROC}/Behavior_new_{uid}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    "    )\n",
    "\n",
    "    row_nr = 30 # number of rows for repetition (was defined in Unity)\n",
    "\n",
    "    # create timestamps\n",
    "    ts = beh_data.index.tolist()\n",
    "    timestamps = np.repeat(ts, row_nr)\n",
    "\n",
    "    # creating beh_data df\n",
    "    beh_cols = [\"valid\", \"leftBlink\", \"rightBlink\"]\n",
    "    b = beh_data\n",
    "    et = np.array([b.valid, b.leftBlink, b.rightBlink])\n",
    "    et = et.T\n",
    "    et_n = np.repeat(et, row_nr, axis=0)\n",
    "    beh_df = pd.DataFrame(data=et_n, columns=beh_cols, index=timestamps)\n",
    "\n",
    "    # create hit_obj df \n",
    "    hon = [] # for HitObjectNames (HON)\n",
    "    hop = [] # HitObjectPositions (HOP) \n",
    "    hpoo = [] # HitPositionOnObjects (HPOO)\n",
    "\n",
    "    # loop through each stream to extract relevant infromation\n",
    "    print(f\"{uid}: sort data\")\n",
    "    for d in data:  # for each stream\n",
    "        s_name = d[\"info\"][\"name\"][0]  # stream name\n",
    "        if s_name == \"HitObjectNames\":\n",
    "            t_hon = times[s_name]\n",
    "            d_hon = d[\"time_series\"]\n",
    "            check = d[\"info\"][\"desc\"][0]\n",
    "            l_hon = None if not check else list(check.keys())\n",
    "        elif s_name == \"HitObjectPositions\":\n",
    "            t_hop = times[s_name]\n",
    "            d_hop = d[\"time_series\"]\n",
    "            check = d[\"info\"][\"desc\"][0]\n",
    "            l_hop = None if not check else list(check.keys())\n",
    "        elif s_name == \"HitPositionOnObjects\":\n",
    "            t_hpoo = times[s_name]\n",
    "            d_hpoo = d[\"time_series\"]\n",
    "            check = d[\"info\"][\"desc\"][0]\n",
    "            l_hpoo = None if not check else list(check.keys())\n",
    "\n",
    "    # HON\n",
    "    print(f\"{uid}: hon\")\n",
    "    hon1 = [\n",
    "        d_hon[[t_hon.index(tim)][0]] if tim in t_hon else [np.nan] * row_nr\n",
    "        for tim in ts\n",
    "    ]\n",
    "    hon = [s for i in range(len(ts)) for s in hon1[i]]\n",
    "    # HOP\n",
    "    print(f\"{uid}: hop\")\n",
    "    hop1 = [\n",
    "        d_hop[[t_hop.index(tim)][0]] if tim in t_hop else [np.nan] * 90\n",
    "        for tim in ts\n",
    "    ]\n",
    "    hop = [hop1[i][s : s + 3] for i in range(len(ts)) for s in range(0, 90, 3)]\n",
    "    # HPOO\n",
    "    print(f\"{uid}: hpoo\")\n",
    "    hpoo1 = [\n",
    "        d_hpoo[[t_hpoo.index(tim)][0]] if tim in t_hpoo else [np.nan] * 90\n",
    "        for tim in ts\n",
    "    ]\n",
    "    hpoo = [\n",
    "        hpoo1[i][s : s + 3] for i in range(len(ts)) for s in range(0, 90, 3)\n",
    "    ]\n",
    "\n",
    "    # create dataframes for HON, HOP, and HPOO\n",
    "    c_hon = pd.DataFrame(data=hon, columns=l_hon, index=timestamps)\n",
    "    c_hop = pd.DataFrame(data=hop, columns=l_hop, index=timestamps)\n",
    "    c_hpoo = pd.DataFrame(data=hpoo, columns=l_hpoo, index=timestamps)\n",
    "\n",
    "    # concatenate dataframes to create the final hit_data\n",
    "    hit_data = pd.concat([beh_df, c_hon, c_hop, c_hpoo], axis=1, join=\"outer\")\n",
    "    hit_data = hit_data.reindex(columns=HIT_COLS)\n",
    "\n",
    "    # save the csv file\n",
    "    hit_data.to_csv(f\"{PATH_PROC}/HitInfo_new_{uid}_raw.csv\", index=True)\n",
    "\n",
    "    # filter and clean hit_data\n",
    "    h_res = hit_data[hit_data.valid == True]\n",
    "    h_res = h_res[h_res.HON != \"Empty\"]\n",
    "    h_res.replace(\"\", np.nan, inplace=True)\n",
    "    h_res.dropna(inplace=True)\n",
    "\n",
    "    # store as CSV (clean)\n",
    "    h_res.to_csv(f\"{PATH_PROC}/HitInfo_new_{uid}.csv\", index=True)\n",
    "\n",
    "\n",
    "# stream names to include\n",
    "INCLUDE = [\n",
    "    \"HitObjectNames\",\n",
    "    \"HitObjectPositions\",\n",
    "    \"HitPositionOnObjects\",\n",
    "]\n",
    "\n",
    "# initialize pool of processes according to the available cpu core threads\n",
    "pool = Pool(processes=CORES, initializer=pbar_fork_hack)\n",
    "\n",
    "# participants ids\n",
    "ids = recordings.index.tolist()\n",
    "idd = ids[:]\n",
    "\n",
    "\n",
    "# participants progress bar\n",
    "parts_pbar = tqdm(\n",
    "    iterable=pool.imap_unordered(func=process_hits, iterable=idd),\n",
    "    total=len(idd),\n",
    "    desc=\"üìÇ participants\",\n",
    "    dynamic_ncols=True,\n",
    "    bar_format=B_FORMAT,\n",
    ")\n",
    "\n",
    "# loop necessary for displaying properly the progressbar with multiprocessing\n",
    "# source: https://stackoverflow.com/a/40133278\n",
    "for _ in parts_pbar:\n",
    "    pass\n",
    "\n",
    "# close pool instance, no more work to submit\n",
    "pool.close()\n",
    "# wait for the worker processes to terminate\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate hit distances at each timepoint and store the shortest one__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "ids = recordings.index.tolist()\n",
    "idd = ids[:]\n",
    "\n",
    "for i, uid in enumerate(idd):\n",
    "    # read processed dataframes\n",
    "    beh_df = pd.read_csv(\n",
    "        f\"{PATH_PROC}/Behavior_new_{uid}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    "    )\n",
    "    # select behavioral data indices of valid raycast only\n",
    "    times = beh_df.index.tolist()\n",
    "    hit_df = pd.read_csv(f\"{PATH_PROC}/HitInfo_new_{uid}.csv\", index_col=0)\n",
    "    h_time = (\n",
    "        hit_df.index.tolist()\n",
    "    )  # timestamps, possibly multiple per timepoint\n",
    "    # to save the index (timestamps) in\n",
    "    to_indx = []\n",
    "    # hits progress bar\n",
    "    times_pbar = tqdm(\n",
    "        iterable=range(len(times)),\n",
    "        desc=f\"‚åö timestamps from participant {uid}\",\n",
    "        dynamic_ncols=True,\n",
    "        bar_format=B_FORMAT,\n",
    "    )\n",
    "    # generate the empty hitdist dict\n",
    "    h_dis = {\"row\": [], \"distance\": []}\n",
    "\n",
    "    # generate te dict for distance\n",
    "    dis_sorted = {\"distance\": []}\n",
    "    # and where the rest is saved\n",
    "    h_sorted = []\n",
    "    for t in times_pbar:  # for each timestamp\n",
    "        tim = times[t]\n",
    "        # select current row in the beh data\n",
    "        b = beh_df.iloc[t]\n",
    "        # select current row in hit data\n",
    "        h = hit_df[hit_df.index == tim]\n",
    "        # origin points (eye-tracking)\n",
    "        eto = np.array([b.ETWoriginX, b.ETWoriginY, b.ETWoriginZ])\n",
    "        # get just the index of them\n",
    "        h_idx = h.index.tolist()\n",
    "        # if there is at least one element in HON\n",
    "        if len(h_idx) > 0:\n",
    "            distances = []\n",
    "            for i in range(len(h)):  # for each row (object name)\n",
    "                cur = h.iloc[i]  # current row\n",
    "                # hit position on object as array (point in 3D space)\n",
    "                hpoo = np.array([cur.HPOOX, cur.HPOOY, cur.HPOOZ])\n",
    "                # calculate distance from ET origin\n",
    "                distance = np.linalg.norm(hpoo - eto)\n",
    "                distances.append(distance)  # store it\n",
    "            if len(distances) > 0:\n",
    "                count = 0\n",
    "                found = True\n",
    "                # closest distance\n",
    "                closest = sorted(set(distances))[count]\n",
    "                idx = distances.index(closest)  # index of closest distance\n",
    "                # taking care of special cases\n",
    "                if (isinstance(idx, str)) and \"BodyCube\" in h.iloc[idx].HON:\n",
    "                    found = False\n",
    "                    count = +1\n",
    "                    # if the body cube is the first seen object --> here use set for length since we might have two identical hits\n",
    "                    if count < len(set(distances)):\n",
    "                        closest = sorted(set(distances))[count]\n",
    "                        idx = distances.index(\n",
    "                            closest\n",
    "                        )  # index of closest distance\n",
    "                        if \"Head\" in h.iloc[idx].HON:\n",
    "                            count += 1\n",
    "                            if count < len(distances):\n",
    "                                closest = sorted(set(distances))[count]\n",
    "                                idx = distances.index(\n",
    "                                    closest\n",
    "                                )  # index of closest distance\n",
    "                                found = True\n",
    "                        else:\n",
    "                            found = True\n",
    "                elif (isinstance(idx, str)) and \"Head\" in h.iloc[idx].HON:\n",
    "                    found = False\n",
    "                    count = +1\n",
    "                    if count < len(distances):\n",
    "                        closest = sorted(set(distances))[count]\n",
    "                        idx = distances.index(\n",
    "                            closest\n",
    "                        )  # index of closest distance\n",
    "                        if \"BodyCube\" in h.iloc[idx].HON:\n",
    "                            count += 1\n",
    "                            if count < len(distances):\n",
    "                                closest = sorted(set(distances))[count]\n",
    "                                idx = distances.index(\n",
    "                                    closest\n",
    "                                )  # index of closest distance\n",
    "                                found = True\n",
    "                        else:\n",
    "                            found = True\n",
    "\n",
    "                if (count + 1) < len(sorted(set(distances))):\n",
    "                    closest2 = sorted(set(distances))[count + 1]\n",
    "                    idx2 = distances.index(\n",
    "                        closest2\n",
    "                    )  # index of closest distance\n",
    "                    if (isinstance(idx, str)) and (\n",
    "                        \"NPC\" in h.iloc[idx].HON\n",
    "                        and \"face\" in h.iloc[idx2].HON\n",
    "                        and int(\"\".join(filter(str.isdigit, h.iloc[idx].HON)))\n",
    "                        == int(\"\".join(filter(str.isdigit, h.iloc[idx2].HON)))\n",
    "                        and abs(closest - closest2) < 1\n",
    "                    ):\n",
    "                        closest = closest2\n",
    "                        idx = idx2\n",
    "                        found = True\n",
    "\n",
    "                if found:\n",
    "                    to_indx = to_indx + [\n",
    "                        h_idx[idx]\n",
    "                    ]  # timestamps to use as index later\n",
    "                    h_dis[\"row\"].append(h_idx[idx])  # store row number (index)\n",
    "                    h_dis[\"distance\"].append(closest)  # store closest distance\n",
    "                    dis_sorted[\"distance\"].append(closest)\n",
    "                    h_sorted.append(h.iloc[idx])\n",
    "\n",
    "    # generate the dataframe of just the object and the distance\n",
    "    hits = pd.DataFrame(h_dis)\n",
    "    hits.to_csv(f\"{PATH_PROC}/HitDistance_new_{uid}.csv\", index=False)\n",
    "\n",
    "    # generate df for the distance and the remaining data and then concatenate them and save as csv\n",
    "    c = pd.DataFrame(dis_sorted, index=to_indx)\n",
    "    hits_sorted = pd.DataFrame(data=h_sorted, columns=HIT_COLS, index=to_indx)\n",
    "    hits_sorted = pd.concat([hits_sorted, c], axis=1, join=\"outer\")\n",
    "    # reorder so that distance is at second position\n",
    "    hits_sorted = hits_sorted.reindex(columns=HIT_SORT)\n",
    "\n",
    "    hits_sorted = hits_sorted[~hits_sorted.index.duplicated(keep=\"first\")]\n",
    "    # hits_sorted = hits_sorted.drop_duplicates(subset =\"First Name\",keep = False, inplace = True)\n",
    "    hits_sorted.to_csv(f\"{PATH_PROC}/HitsSorted_new_{uid}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_hits(uid):\n",
    "    \"\"\"\n",
    "    Calculate and store hit information data given a recording id.\n",
    "\n",
    "    Parameters:\n",
    "        uid (str): Participant identifier.\n",
    "    \"\"\"\n",
    "    # read processed dataframes\n",
    "    beh_df = pd.read_csv(\n",
    "        f\"{PATH_PROC}/Behavior_new_{uid}.csv\", index_col=0, dtype=CUSTOM_DTYPES\n",
    "    )\n",
    "    # select behavioral data indices of valid raycast only\n",
    "    times = beh_df.index.tolist()\n",
    "    hit_df = pd.read_csv(f\"{PATH_PROC}/HitInfo_new_{uid}.csv\", index_col=0)\n",
    "    h_time = (\n",
    "        hit_df.index.tolist()\n",
    "    )  # timestamps, possibly multiple per timepoint\n",
    "    # to save the index (timestamps) in\n",
    "    to_indx = []\n",
    "\n",
    "    # hits progress bar\n",
    "    times_pbar = tqdm(\n",
    "        iterable=range(len(times)),\n",
    "        desc=f\"‚åö timestamps from participant {uid}\",\n",
    "        dynamic_ncols=True,\n",
    "        bar_format=B_FORMAT,\n",
    "    )\n",
    "\n",
    "    # generate the empty hitdist dict\n",
    "    h_dis = {\"row\": [], \"distance\": []}\n",
    "\n",
    "    # generate te dict for distance\n",
    "    dis_sorted = {\"distance\": []}\n",
    "    # and where the rest is saved\n",
    "    h_sorted = []\n",
    "    for t in times_pbar:  # for each timestamp\n",
    "        tim = times[t]\n",
    "        # select current row in the beh data\n",
    "        b = beh_df.iloc[t]\n",
    "        # select current row in hit data\n",
    "        h = hit_df[hit_df.index == tim]\n",
    "        # origin points (eye-tracking)\n",
    "        eto = np.array([b.ETWoriginX, b.ETWoriginY, b.ETWoriginZ])\n",
    "        # get just the index of them\n",
    "        h_idx = h.index.tolist()\n",
    "        # if there is at least one element in HON\n",
    "        if len(h_idx) > 0:\n",
    "            distances = []\n",
    "            for i in range(len(h)):  # for each row (object name)\n",
    "                cur = h.iloc[i]  # current row\n",
    "                # hit position on object as array (point in 3D space)\n",
    "                hpoo = np.array([cur.HPOOX, cur.HPOOY, cur.HPOOZ])\n",
    "                # calculate distance from ET origin\n",
    "                distance = np.linalg.norm(hpoo - eto)\n",
    "                distances.append(distance)  # store it\n",
    "            if len(distances) > 0:\n",
    "                count = 0\n",
    "                found = True\n",
    "                # closest distance\n",
    "                closest = sorted(set(distances))[count]\n",
    "                idx = distances.index(closest)  # index of closest distance\n",
    "                if (isinstance(idx, str)) and \"BodyCube\" in h.iloc[idx].HON:\n",
    "                    found = False\n",
    "                    count = +1\n",
    "                    # if the body cube is the first seen object --> here use set for length since we might have two identical hits\n",
    "                    if count < len(set(distances)):\n",
    "                        closest = sorted(set(distances))[count]\n",
    "                        idx = distances.index(\n",
    "                            closest\n",
    "                        )  # index of closest distance\n",
    "                        if \"Head\" in h.iloc[idx].HON:\n",
    "                            count += 1\n",
    "                            if count < len(distances):\n",
    "                                closest = sorted(set(distances))[count]\n",
    "                                idx = distances.index(\n",
    "                                    closest\n",
    "                                )  # index of closest distance\n",
    "                                found = True\n",
    "                        else:\n",
    "                            found = True\n",
    "                elif (isinstance(idx, str)) and \"Head\" in h.iloc[idx].HON:\n",
    "                    found = False\n",
    "                    count = +1\n",
    "                    if count < len(distances):\n",
    "                        closest = sorted(set(distances))[count]\n",
    "                        idx = distances.index(\n",
    "                            closest\n",
    "                        )  # index of closest distance\n",
    "                        if \"BodyCube\" in h.iloc[idx].HON:\n",
    "                            count += 1\n",
    "                            if count < len(distances):\n",
    "                                closest = sorted(set(distances))[count]\n",
    "                                idx = distances.index(\n",
    "                                    closest\n",
    "                                )  # index of closest distance\n",
    "                                found = True\n",
    "                        else:\n",
    "                            found = True\n",
    "\n",
    "                if (count + 1) < len(sorted(set(distances))):\n",
    "                    closest2 = sorted(set(distances))[count + 1]\n",
    "                    idx2 = distances.index(\n",
    "                        closest2\n",
    "                    )  # index of closest distance\n",
    "                    if (isinstance(idx, str)) and (\n",
    "                        \"NPC\" in h.iloc[idx].HON\n",
    "                        and \"face\" in h.iloc[idx2].HON\n",
    "                        and int(\"\".join(filter(str.isdigit, h.iloc[idx].HON)))\n",
    "                        == int(\"\".join(filter(str.isdigit, h.iloc[idx2].HON)))\n",
    "                        and abs(closest - closest2) < 1\n",
    "                    ):\n",
    "                        closest = closest2\n",
    "                        idx = idx2\n",
    "                        found = True\n",
    "                # if found, store the relevant information\n",
    "                if found:\n",
    "                    to_indx = to_indx + [\n",
    "                        h_idx[idx]\n",
    "                    ]  # timestamps to use as index later\n",
    "                    h_dis[\"row\"].append(h_idx[idx])  # store row number (index)\n",
    "                    h_dis[\"distance\"].append(closest)  # store closest distance\n",
    "                    dis_sorted[\"distance\"].append(closest)\n",
    "                    h_sorted.append(h.iloc[idx])\n",
    "\n",
    "    # generate the dataframe of just the object and the distance\n",
    "    hits = pd.DataFrame(h_dis)\n",
    "    hits.to_csv(f\"{PATH_PROC}/HitDistance_new_{uid}.csv\", index=False)\n",
    "\n",
    "    # generate df for the distance and the remaining data and then concatenate them and save as csv\n",
    "    c = pd.DataFrame(dis_sorted, index=to_indx)\n",
    "    hits_sorted = pd.DataFrame(data=h_sorted, columns=HIT_COLS, index=to_indx)\n",
    "    hits_sorted = pd.concat([hits_sorted, c], axis=1, join=\"outer\")\n",
    "    # reorder so that distance is at second position\n",
    "    hits_sorted = hits_sorted.reindex(columns=HIT_SORT)\n",
    "\n",
    "    hits_sorted = hits_sorted[~hits_sorted.index.duplicated(keep=\"first\")]\n",
    "    # hits_sorted = hits_sorted.drop_duplicates(subset =\"First Name\",keep = False, inplace = True)\n",
    "    hits_sorted.to_csv(f\"{PATH_PROC}/HitsSorted_new_{uid}.csv\", index=True)\n",
    "\n",
    "\n",
    "# stream names to include\n",
    "INCLUDE = [\n",
    "    \"HitObjectNames\",\n",
    "    \"HitObjectPositions\",\n",
    "    \"HitPositionOnObjects\",\n",
    "]\n",
    "# initialize pool of processes according to the available cpu core threads\n",
    "pool = Pool(processes=CORES, initializer=pbar_fork_hack)\n",
    "\n",
    "# participants ids\n",
    "ids = recordings.index.tolist()\n",
    "idd = ids[:]\n",
    "\n",
    "\n",
    "# participants progress bar\n",
    "parts_pbar = tqdm(\n",
    "    iterable=pool.imap_unordered(func=calculate_hits, iterable=idd),\n",
    "    total=len(idd),\n",
    "    desc=\"üìÇ participants\",\n",
    "    dynamic_ncols=True,\n",
    "    bar_format=B_FORMAT,\n",
    ")\n",
    "\n",
    "# loop necessary for displaying properly the progressbar with multiprocessing\n",
    "# source: https://stackoverflow.com/a/40133278\n",
    "for _ in parts_pbar:\n",
    "    pass\n",
    "\n",
    "# close pool instance, no more work to submit\n",
    "pool.close()\n",
    "# wait for the worker processes to terminate\n",
    "pool.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wd_village)",
   "language": "python",
   "name": "wd_village"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
